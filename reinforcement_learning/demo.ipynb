{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case demonstration for supervised, unsupervised, and reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "AI and ML are transforming various industries by enabling machines to learn from data and make predictions or decisions. This reading will delve into three primary learning paradigms: supervised learning, unsupervised learning, and RL. By exploring practical use cases, you will gain insights into how these approaches function in real-world scenarios, equipping you with the knowledge to apply these concepts effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning: Predicting house prices\n",
    "\n",
    "## Use case overview\n",
    "\n",
    "In a supervised learning scenario, we aim to predict house prices based on features such as square footage, the number of bedrooms, and the neighborhood. We have a dataset where each house is labeled with its corresponding price, making it a classic supervised learning regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1745615127.45857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data (square footage, bedrooms, neighborhood as encoded values)\n",
    "X = [[2000, 3, 1], [1500, 2, 2], [1800, 3, 3], [1200, 2, 1]]\n",
    "y = [500000, 350000, 450000, 300000]\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning: Customer segmentation\n",
    "\n",
    "## Use case overview\n",
    "\n",
    "For a business trying to understand customer behavior, unsupervised learning can be applied to segment customers based on their purchasing habits. The company wants to group similar customers together to better target marketing strategies, but they do not have predefined labels for these customer segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [[3.5e+00 7.5e+02 1.5e+00]\n",
      " [9.0e+00 4.0e+03 4.0e+00]]\n",
      "Labels: [0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample customer data (number of purchases, total spending, product categories)\n",
    "X = np.array([[5, 1000, 2], [10, 5000, 5], [2, 500, 1], [8, 3000, 3]])\n",
    "\n",
    "# Create and fit the KMeans model\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "\n",
    "# Print the cluster centers and labels\n",
    "print(f\"Cluster Centers: {kmeans.cluster_centers_}\")\n",
    "print(f\"Labels: {kmeans.labels_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL: Training an agent to play tic-tac-toe\n",
    "\n",
    "## Use case overview\n",
    "\n",
    "In this RL scenario, we train an agent to play the game of tic-tac-toe. The agent interacts with the game environment by placing its marks (X or O) on the board and receives rewards for winning (+1), losing (-1), or drawing (0). There are no labeled data or predefined strategies; the agent must learn through trial and error to improve its gameplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m9\u001b[39m)  \u001b[38;5;66;03m# Simulate next state\u001b[39;00m\n\u001b[1;32m     32\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39mรง1 \u001b[38;5;28;01mif\u001b[39;00m next_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Simulate rewards\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mupdate_q_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36mupdate_q_table\u001b[0;34m(state, action, reward, next_state, Q_table)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_q_table\u001b[39m(state, action, reward, next_state, Q_table):\n\u001b[1;32m     20\u001b[0m     Q_table[state, action] \u001b[38;5;241m=\u001b[39m Q_table[state, action] \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m---> 21\u001b[0m         reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m Q_table[state, action]\n\u001b[1;32m     22\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/work/learning/ml_project/venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3164\u001b[0m, in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[1;32m   3053\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   3055\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   3056\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   3058\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   3163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/work/learning/ml_project/venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize Q-table with zeros for all state-action pairs\n",
    "Q_table = np.zeros((9, 9))  # 9 possible states (board positions) and 9 possible actions\n",
    "\n",
    "# Learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Sample function to select action using epsilon-greedy policy\n",
    "def epsilon_greedy_action(state, Q_table, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(0, 9)  # Random action (explore)\n",
    "    else:\n",
    "        return np.argmax(Q_table[state])  # Best action (exploit)\n",
    "\n",
    "# Update Q-values after each game (simplified example)\n",
    "def update_q_table(state, action, reward, next_state, Q_table):\n",
    "    Q_table[state, action] = Q_table[state, action] + alpha * (\n",
    "        reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action]\n",
    "    )\n",
    "\n",
    "# Example simulation of a game where the agent learns\n",
    "for episode in range(1000):\n",
    "    print('episode', episode)\n",
    "    state = np.random.randint(0, 9)  # Random initial state\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(state, Q_table, epsilon)\n",
    "        next_state = np.random.randint(0, 9)  # Simulate next state\n",
    "        reward = 1 if next_state == 'win' else -รง1 if next_state == 'loss' else 0  # Simulate rewards\n",
    "        update_q_table(state, action, reward, next_state, Q_table)\n",
    "        state = next_state\n",
    "        if reward != 0:\n",
    "            done = True  # End the game if win/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
