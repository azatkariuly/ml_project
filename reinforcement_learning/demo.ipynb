{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case demonstration for supervised, unsupervised, and reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "AI and ML are transforming various industries by enabling machines to learn from data and make predictions or decisions. This reading will delve into three primary learning paradigms: supervised learning, unsupervised learning, and RL. By exploring practical use cases, you will gain insights into how these approaches function in real-world scenarios, equipping you with the knowledge to apply these concepts effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning: Predicting house prices\n",
    "\n",
    "## Use case overview\n",
    "\n",
    "In a supervised learning scenario, we aim to predict house prices based on features such as square footage, the number of bedrooms, and the neighborhood. We have a dataset where each house is labeled with its corresponding price, making it a classic supervised learning regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1745615127.45857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data (square footage, bedrooms, neighborhood as encoded values)\n",
    "X = [[2000, 3, 1], [1500, 2, 2], [1800, 3, 3], [1200, 2, 1]]\n",
    "y = [500000, 350000, 450000, 300000]\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning: Customer segmentation\n",
    "\n",
    "## Use case overview\n",
    "\n",
    "For a business trying to understand customer behavior, unsupervised learning can be applied to segment customers based on their purchasing habits. The company wants to group similar customers together to better target marketing strategies, but they do not have predefined labels for these customer segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [[3.5e+00 7.5e+02 1.5e+00]\n",
      " [9.0e+00 4.0e+03 4.0e+00]]\n",
      "Labels: [0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample customer data (number of purchases, total spending, product categories)\n",
    "X = np.array([[5, 1000, 2], [10, 5000, 5], [2, 500, 1], [8, 3000, 3]])\n",
    "\n",
    "# Create and fit the KMeans model\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "\n",
    "# Print the cluster centers and labels\n",
    "print(f\"Cluster Centers: {kmeans.cluster_centers_}\")\n",
    "print(f\"Labels: {kmeans.labels_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL: Training an agent to play tic-tac-toe\n",
    "\n",
    "## Use case overview\n",
    "\n",
    "In this RL scenario, we train an agent to play the game of tic-tac-toe. The agent interacts with the game environment by placing its marks (X or O) on the board and receives rewards for winning (+1), losing (-1), or drawing (0). There are no labeled data or predefined strategies; the agent must learn through trial and error to improve its gameplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize Q-table with zeros for all state-action pairs\n",
    "Q_table = np.zeros((9, 9))  # 9 possible states (board positions) and 9 possible actions\n",
    "\n",
    "# Learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Sample function to select action using epsilon-greedy policy\n",
    "def epsilon_greedy_action(state, Q_table, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(0, 9)  # Random action (explore)\n",
    "    else:\n",
    "        return np.argmax(Q_table[state])  # Best action (exploit)\n",
    "\n",
    "# Update Q-values after each game (simplified example)\n",
    "def update_q_table(state, action, reward, next_state, Q_table):\n",
    "    Q_table[state, action] = Q_table[state, action] + alpha * (\n",
    "        reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action]\n",
    "    )\n",
    "\n",
    "# Example simulation of a game where the agent learns\n",
    "for episode in range(1000):\n",
    "    print('episode', episode)\n",
    "    state = np.random.randint(0, 9)  # Random initial state\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(state, Q_table, epsilon)\n",
    "        next_state = np.random.randint(0, 9)  # Simulate next state\n",
    "        reward = 1 if next_state == 'win' else -รง1 if next_state == 'loss' else 0  # Simulate rewards\n",
    "        update_q_table(state, action, reward, next_state, Q_table)\n",
    "        state = next_state\n",
    "        if reward != 0:\n",
    "            done = True  # End the game if win/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
